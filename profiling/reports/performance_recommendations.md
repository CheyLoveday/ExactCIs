# Recommendations for Accurate Performance Measurement and Reporting

## Key Findings Summary

Our investigation into the conditional method optimization revealed:

1. **Significant discrepancies in performance claims**:
   - Claimed: 5.20ms â†’ 0.42ms (92% improvement)
   - Actual: ~230ms first run, ~8-9ms subsequent runs

2. **Optimizations are correctly implemented and effective**:
   - Shared cache system works as intended
   - Pre-bracketing improves root-finding efficiency
   - Performance scales well with table size

3. **Cache initialization has a major impact on performance**:
   - First execution is ~25x slower than subsequent executions
   - This effect was not documented in the optimization summary

## Recommendations for Accurate Performance Measurement

1. **Standardize Benchmark Methodology**:
   - **Warm-up runs**: Always include at least one warm-up run before timing
   - **Multiple measurements**: Report average of multiple runs (minimum 3)
   - **Cache state**: Clearly document whether cache is warm or cold
   - **Table sizes**: Test with consistent table sizes across all benchmarks
   - **Environment**: Document hardware, OS, and library versions

2. **Report Complete Performance Metrics**:
   - **First-run performance**: Report cold-cache performance
   - **Subsequent-run performance**: Report warm-cache performance
   - **Variability**: Include min/max/std deviation
   - **Scaling behavior**: Show how performance scales with table size

3. **Implement Automated Benchmarking**:
   - Create a standardized benchmark suite that runs automatically
   - Store benchmark results in a structured format
   - Track performance over time to detect regressions
   - Include benchmarks in CI/CD pipeline

4. **Document Performance Characteristics**:
   - Explain cache behavior and initialization costs
   - Provide guidance on when to pre-warm cache
   - Document expected scaling behavior
   - Note any environment-specific performance considerations

5. **Validate Claims with Multiple Environments**:
   - Test on different hardware configurations
   - Test with different Python/NumPy/SciPy versions
   - Document performance variations across environments

## Implementation Suggestions

1. **Create a Benchmark Harness**:
   ```python
   import time
   import statistics
   
   def benchmark_method(method_func, args, warm_up_runs=1, timed_runs=5):
       """Standard benchmark harness for consistent measurements."""
       # Warm-up runs
       for _ in range(warm_up_runs):
           method_func(*args)
       
       # Timed runs
       times = []
       for _ in range(timed_runs):
           start = time.perf_counter()
           result = method_func(*args)
           end = time.perf_counter()
           times.append((end - start) * 1000)  # ms
       
       return {
           'mean': statistics.mean(times),
           'median': statistics.median(times),
           'min': min(times),
           'max': max(times),
           'std_dev': statistics.stdev(times) if len(times) > 1 else 0
       }
   ```

2. **Add Cache Pre-warming Option**:
   ```python
   from exactcis.methods.conditional import exact_ci_conditional
   
   def prewarm_cache(table_sizes=None):
       """Pre-warm the cache with common table sizes."""
       if table_sizes is None:
           table_sizes = [
               (2, 3, 4, 5),      # very small
               (10, 15, 12, 18),   # medium
               (50, 75, 60, 90),   # large
           ]
       
       for a, b, c, d in table_sizes:
           exact_ci_conditional(a, b, c, d)
       
       return "Cache pre-warmed with standard table sizes"
   ```

3. **Performance Documentation Template**:
   ```markdown
   ## Performance Characteristics
   
   | Method | Cold Cache | Warm Cache | Improvement |
   |--------|------------|------------|-------------|
   | Method | XX.XX ms   | XX.XX ms   | XX%         |
   
   ### Environment
   - Python: X.X.X
   - NumPy: X.X.X
   - SciPy: X.X.X
   - OS: XXX
   - CPU: XXX
   
   ### Scaling Behavior
   | Table Size | N    | Execution Time |
   |------------|------|----------------|
   | Small      | XX   | XX.XX ms       |
   | Medium     | XXX  | XX.XX ms       |
   | Large      | XXXX | XX.XX ms       |
   ```

## Conclusion

Accurate performance measurement and reporting are essential for making informed optimization decisions. By implementing these recommendations, the ExactCIs library can provide more transparent and reliable performance information to users and developers.

The current optimizations to the conditional method are effective, but the performance claims should be updated to reflect actual measurements, including the impact of cache initialization and the performance after warm-up.

Date: August 4, 2025