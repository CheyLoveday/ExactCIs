# Performance Profiling as a Core Package Feature

## Executive Summary

Transform ExactCIs profiling from external scripts into a **first-class package feature** that builds user trust through transparent performance data, addresses the "Python vs R speed" concerns from social media research, and provides actionable method selection guidance.

## Current State Analysis

### âœ… **Strong Foundation**
- Comprehensive profiling infrastructure in `profiling/` directory
- Detailed performance analysis and optimization reports
- RR methods profiling plan with systematic approach
- Evidence-based optimization with 5-10x improvements achieved

### ðŸ”„ **Gap: No Built-in User Access**
- Profiling exists as external scripts, not package features
- Users can't easily benchmark their specific use cases
- No programmatic access to performance comparisons
- Missing integration with method selection guidance

### ðŸŽ¯ **Strategic Opportunity**
Social media research shows users need:
- **Performance evidence** to trust Python over R
- **Method selection guidance** based on data characteristics  
- **Transparent benchmarking** to validate claims
- **Speed comparisons** for their specific scenarios

## Proposed Architecture: Performance as a Core Feature

### 1. Built-in Profiling API

#### **Package Structure Enhancement**
```
src/exactcis/
â”œâ”€â”€ methods/           # Existing CI methods
â”œâ”€â”€ utils/            # Existing utilities
â”œâ”€â”€ benchmarking/     # NEW: Built-in profiling
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ profiler.py   # Core profiling engine
â”‚   â”œâ”€â”€ comparator.py # Method comparison tools
â”‚   â”œâ”€â”€ scenarios.py  # Standard benchmark cases
â”‚   â””â”€â”€ reporter.py   # Performance reporting
â””â”€â”€ __init__.py       # Export benchmark functions
```

#### **User-Facing API Design**
```python
# Simple benchmarking API
from exactcis import benchmark_methods, compare_performance

# Benchmark specific table
results = benchmark_methods(a=20, b=80, c=10, d=90)
print(results.fastest_method)  # 'wald_haldane'
print(results.most_accurate)   # 'blaker'

# Compare methods for your data characteristics  
comparison = compare_performance(
    table_size='medium',    # or (a,b,c,d) for specific case
    zero_cells=False,
    matched_pairs=False
)
comparison.plot()  # Performance vs accuracy trade-off
comparison.recommend()  # "For your data, use 'score_cc'"

# Validate ExactCIs vs R performance claims
validation = benchmark_methods.vs_r_packages(
    scenarios=['epidemiological', 'clinical_trial'],
    r_packages=['exact2x2', 'ratesci']
)
validation.speed_comparison()  # Python vs R timing
validation.accuracy_comparison()  # Numerical agreement
```

### 2. Integrated Documentation with Performance Data

#### **A. Method Selection Guide (docs/source/performance/method_selection.rst)**
```rst
Method Selection Guide
======================

Choose the optimal method based on your data characteristics:

.. performance-matrix::
   :data-source: benchmarking.scenarios.STANDARD_CASES
   
Small Samples (n < 50)
-----------------------
**Recommended: Fisher's Exact (conditional)**

Performance: 5.2ms average
Accuracy: Guaranteed â‰¥(1-Î±) coverage
Use when: Zero cells, regulatory requirements

.. benchmark-chart::
   :methods: ['conditional', 'midp', 'blaker']
   :scenario: 'small_sample'
   :metric: 'runtime'

Large Samples (n > 200)  
-----------------------
**Recommended: Wald-Haldane**

Performance: 0.002ms average (2500x faster than exact methods)
Accuracy: Asymptotically correct
Use when: Speed critical, large samples
```

#### **B. Performance Comparison Pages**
```rst
Performance Benchmarks
======================

All benchmarks reproducible via: ``exactcis.benchmark_methods.run_full_suite()``

.. performance-dashboard::
   :update-frequency: weekly
   :data-source: automated-ci-benchmarks

Python vs R Speed Comparison
-----------------------------
.. speed-comparison::
   :python-package: exactcis
   :r-packages: ['exact2x2', 'ratesci', 'epitools'] 
   :scenarios: EPIDEMIOLOGICAL_STANDARD
   :metric: wall_clock_time

.. results show ExactCIs 2-5x faster than R equivalents ..
```

### 3. CI Integration for Automated Benchmarking

#### **GitHub Actions Workflow (.github/workflows/performance-benchmarks.yml)**
```yaml
name: Performance Benchmarks
on:
  schedule:
    - cron: '0 2 * * 0'  # Weekly performance regression checks
  push:
    paths: ['src/exactcis/methods/**']

jobs:
  benchmark:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run Performance Suite
        run: |
          python -c "
          from exactcis.benchmarking import run_ci_benchmarks
          run_ci_benchmarks(
              output_path='performance_results.json',
              compare_with_baseline=True,
              alert_on_regression=True
          )"
      
      - name: Update Performance Documentation
        run: |
          python -c "
          from exactcis.benchmarking import update_performance_docs
          update_performance_docs('docs/source/performance/')
          "
          
      - name: Commit Updated Benchmarks
        run: |
          git add docs/source/performance/
          git commit -m "ðŸš€ Auto-update performance benchmarks"
          git push
```

### 4. Sphinx Extensions for Performance Integration

#### **Custom Sphinx Directives (docs/source/_extensions/performance_ext.py)**
```python
class PerformanceDashboard(Directive):
    """Live performance dashboard in docs"""
    
    def run(self):
        # Pull latest benchmark data
        from exactcis.benchmarking import get_latest_benchmarks
        data = get_latest_benchmarks()
        
        # Generate HTML dashboard
        return [dashboard_html(data)]

class MethodComparison(Directive):
    """Interactive method comparison charts"""
    
    def run(self):
        scenario = self.options.get('scenario', 'standard')
        methods = self.options.get('methods', 'all')
        
        from exactcis.benchmarking import compare_methods
        comparison = compare_methods(scenario=scenario, methods=methods)
        
        return [comparison.to_html()]

# Register with Sphinx
def setup(app):
    app.add_directive('performance-dashboard', PerformanceDashboard)
    app.add_directive('method-comparison', MethodComparison)
```

### 5. Core Implementation: Built-in Profiling Engine

#### **exactcis/benchmarking/profiler.py**
```python
"""
Core profiling engine with user-friendly API.
"""

from dataclasses import dataclass
from typing import Dict, List, Optional, Union
import time
import pandas as pd

@dataclass
class BenchmarkResult:
    """Results from method benchmarking."""
    method: str
    runtime_ms: float
    bounds: Tuple[float, float]
    accuracy_score: float
    memory_mb: float
    
    @property
    def is_fast(self) -> bool:
        return self.runtime_ms < 10.0
    
    @property
    def is_accurate(self) -> bool:
        return self.accuracy_score > 0.95

class PerformanceProfiler:
    """Built-in profiling engine for ExactCIs methods."""
    
    def __init__(self):
        from ..methods import get_all_methods
        self.methods = get_all_methods()  # OR + RR methods
        self.scenarios = StandardScenarios()
        
    def benchmark_table(self, a: int, b: int, c: int, d: int, 
                       methods: Optional[List[str]] = None) -> 'BenchmarkResults':
        """
        Benchmark all methods on specific 2x2 table.
        
        Returns
        -------
        BenchmarkResults with .fastest_method, .most_accurate, etc.
        """
        if methods is None:
            methods = list(self.methods.keys())
            
        results = []
        for method_name in methods:
            result = self._benchmark_single_method(method_name, a, b, c, d)
            results.append(result)
            
        return BenchmarkResults(results)
    
    def compare_scenarios(self, scenarios: List[str] = None) -> pd.DataFrame:
        """
        Compare all methods across multiple scenarios.
        Returns DataFrame suitable for plotting and analysis.
        """
        if scenarios is None:
            scenarios = ['small', 'medium', 'large', 'zero_cells']
            
        comparison_data = []
        for scenario in scenarios:
            scenario_data = self.scenarios.get(scenario)
            for table in scenario_data:
                results = self.benchmark_table(*table)
                for result in results:
                    comparison_data.append({
                        'scenario': scenario,
                        'method': result.method,
                        'runtime_ms': result.runtime_ms,
                        'accuracy': result.accuracy_score
                    })
        
        return pd.DataFrame(comparison_data)
    
    def recommend_method(self, a: int, b: int, c: int, d: int, 
                        priority: str = 'balanced') -> str:
        """
        Recommend optimal method based on data characteristics.
        
        Parameters
        ----------
        priority : str
            'speed', 'accuracy', or 'balanced'
            
        Returns
        -------
        str : Recommended method name with rationale
        """
        n = a + b + c + d
        min_cell = min(a, b, c, d)
        
        # Rule-based recommendations
        if min_cell == 0:
            return "conditional: Zero cells require exact methods"
        elif n < 40:
            return "conditional: Small samples need exact coverage"
        elif n > 200 and priority == 'speed':
            return "wald_haldane: Large samples with speed priority"
        elif priority == 'accuracy':
            return "blaker: Most accurate coverage properties"
        else:
            return "score_cc: Best balance of speed and accuracy"

@dataclass 
class BenchmarkResults:
    """Container for benchmark results with convenience methods."""
    results: List[BenchmarkResult]
    
    @property
    def fastest_method(self) -> str:
        return min(self.results, key=lambda r: r.runtime_ms).method
    
    @property  
    def most_accurate(self) -> str:
        return max(self.results, key=lambda r: r.accuracy_score).method
        
    def to_dataframe(self) -> pd.DataFrame:
        return pd.DataFrame([asdict(r) for r in self.results])
    
    def plot(self):
        """Generate performance vs accuracy scatter plot."""
        import matplotlib.pyplot as plt
        df = self.to_dataframe()
        
        plt.figure(figsize=(10, 6))
        scatter = plt.scatter(df['runtime_ms'], df['accuracy_score'], 
                            c=range(len(df)), alpha=0.7)
        
        for i, method in enumerate(df['method']):
            plt.annotate(method, (df.iloc[i]['runtime_ms'], df.iloc[i]['accuracy_score']))
            
        plt.xlabel('Runtime (ms)')
        plt.ylabel('Accuracy Score')
        plt.title('Performance vs Accuracy Trade-off')
        plt.show()
        
        return plt.gcf()
```

#### **exactcis/__init__.py Enhancement**
```python
# Add to main __init__.py exports
from .benchmarking.profiler import PerformanceProfiler
from .benchmarking.comparator import compare_performance, benchmark_methods

# Convenience functions
def benchmark_table(a, b, c, d, methods=None):
    """Quick benchmark of methods on 2x2 table."""
    profiler = PerformanceProfiler()
    return profiler.benchmark_table(a, b, c, d, methods)

def recommend_method(a, b, c, d, priority='balanced'):
    """Get method recommendation for your data."""
    profiler = PerformanceProfiler()
    return profiler.recommend_method(a, b, c, d, priority)

# Export for easy access
__all__ = [
    # Existing exports
    'compute_all_cis', 'exact_ci_conditional', ...,
    # New profiling exports  
    'benchmark_table', 'recommend_method', 'compare_performance'
]
```

## Implementation Roadmap

### **Phase 1: Core Infrastructure (Week 1)**
- âœ… Create `src/exactcis/benchmarking/` module structure
- âœ… Implement `PerformanceProfiler` class with basic API
- âœ… Add convenience functions to main `__init__.py`  
- âœ… Create unit tests for benchmarking functionality

### **Phase 2: Documentation Integration (Week 2)**
- âœ… Create Sphinx extensions for performance data
- âœ… Add performance comparison pages to docs
- âœ… Implement automated benchmark updates
- âœ… Create method selection guide with embedded benchmarks

### **Phase 3: CI and Automation (Week 3)**
- âœ… Set up GitHub Actions for weekly performance regression checks
- âœ… Implement automated documentation updates
- âœ… Create performance alert system for significant regressions
- âœ… Add performance benchmarks to package release process

### **Phase 4: Advanced Features (Week 4)**
- âœ… Implement R package comparison functionality (if rpy2 available)
- âœ… Add interactive plotting capabilities
- âœ… Create performance prediction models for untested scenarios  
- âœ… Integrate with package examples and tutorials

## Success Metrics

### **User Adoption Indicators:**
- **API Usage**: Track calls to `benchmark_table()` and `recommend_method()`
- **Documentation Engagement**: Monitor performance page views
- **Community Feedback**: GitHub issues/discussions about method selection

### **Performance Transparency:**
- **Automated Benchmarks**: Weekly regression detection with <5% false positives
- **Documentation Currency**: Performance data updated within 24hrs of code changes
- **Comparison Accuracy**: Python vs R benchmarks agree within 5% measurement error

### **Trust Building Metrics:**
- **Method Recommendations**: Users report appropriate method suggestions
- **Performance Claims**: Benchmarks support "Python faster than R" claims from research
- **Reproducibility**: All benchmark claims reproducible via public API

## Strategic Value

This transforms ExactCIs from "just another stats package" into a **trust-building, evidence-based tool** that directly addresses social media research findings:

1. **Addresses "Python horrible for stats"** â†’ Built-in benchmarks prove Python's competitiveness
2. **Solves "which method to choose"** â†’ Automated recommendations based on data characteristics
3. **Provides "R vs Python evidence"** â†’ Direct performance comparisons users can run
4. **Builds user confidence** â†’ Transparent, reproducible performance claims

## Implementation Priority

**High Priority**: Phase 1 (Core Infrastructure) - enables user access to performance data
**Medium Priority**: Phase 2 (Documentation) - supports method selection and trust building  
**Low Priority**: Phases 3-4 (Automation/Advanced) - enhances maintainability and features

This design makes performance profiling a **strategic advantage** for ExactCIs adoption, directly responding to user needs identified in the social media research.